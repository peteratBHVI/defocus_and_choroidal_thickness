{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Author: p.wagner@bhvi.org / p.wagner@unsw.edu.au \n",
    "image registration of anterior segment projection from several scans \n",
    "\n",
    "Purpose: \n",
    "\n",
    "statistical analyese of choroid thickness maps after intra and inter participant alignment\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "from pystackreg import StackReg\n",
    "from skimage import io\n",
    "from scipy import stats\n",
    "\n",
    "import natsort\n",
    "import sys\n",
    "sys.path.append(r'C:\\Users\\p.wagner\\Documents\\Python Scripts\\oct_data_analyses_helpers')\n",
    "from oct_helpers_lib import OctDataAccess as get_px_meta\n",
    "from oct_helpers_lib import TopconSegmentationData as TSD\n",
    "\n",
    "import cv2\n",
    "import glob\n",
    "from PIL import Image\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "path_oct = r'E:\\studyIII\\OCT_data'\n",
    "path_logbook = r'C:\\Users\\p.wagner\\Documents\\phd\\C4 study_III\\participants'\n",
    "fn_logbook = 'participant_log_studyIII_V0.2.xlsx' \n",
    "fp_fn_logbook = os.path.join(path_logbook, fn_logbook)\n",
    "thickness_csv = 'DEFAULT_3D_All_Thickness.csv'\n",
    "\n",
    "\n",
    "# check if path_oct is available \n",
    "if not os.path.isdir(path_oct):\n",
    "    print('OCT data path NOT available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_data_fp = r'E:\\studyIII\\OCT_final_data'\n",
    "def get_choroid_thickness_data(px_ids, eye_id):\n",
    "    # raw data\n",
    "#     residuals_fn = str(px_id) +'_' + eye_id + '_choroid_thickness_map_mm_raw.csv'\n",
    "    # filtered data \n",
    "#     residuals_fn = str(px_id) +'_' + eye_id + '_choroid_thickness_map_mm_std_filter.csv'\n",
    "    nan_counter = []\n",
    "    layer_all = []   \n",
    "    for px_id in px_ids:\n",
    "        residuals_fn = str(px_id) +'_' + eye_id + '_choroid_thickness_map_mm_std_filter.csv'\n",
    "        fpn = os.path.join(residuals_data_fp, residuals_fn)\n",
    "#         print(fpn)\n",
    "        r_map = pd.read_csv(fpn, index_col=0).values.astype(float) \n",
    "        layer_all.append(r_map)\n",
    "        \n",
    "        # count all pixel positions with NO depth data \n",
    "        idxs = np.where(np.isnan(r_map)) \n",
    "        mask = np.zeros(r_map.shape)\n",
    "        mask[idxs[0], idxs[1]] = 1\n",
    "        nan_counter.append(mask)\n",
    "        \n",
    "    layer_all = np.array(layer_all)    \n",
    "    nan_counter = np.nansum(np.array(nan_counter), axis=0)\n",
    "    \n",
    "    return layer_all, nan_counter\n",
    "\n",
    "def map_significant_data(display_data, save_file=False, output_fpn=None):\n",
    "    # creating a accumulative map of dioptric landscape\n",
    "    fig = px.imshow(display_data,\n",
    "                    title=title_name,\n",
    "                    labels=dict(x='Horizontal decentration from fovea in degrees [°]',\n",
    "                                y='Vertical decentration from fovea in degrees [°]',\n",
    "                                color='Thickness change [μm]',\n",
    "                                ),\n",
    "#                     # pixels to degrees translation\n",
    "                    x=np.array([float(x) * 0.0703 for x in range(0, 512)]) - 18,\n",
    "                    y=-1 * np.array([float(x) * 0.10547 for x in range(0, 256)]) + 13.5,\n",
    "#                         y = 1 * np.array([int(x) for x in range(0, 256)])\n",
    "                    )\n",
    "    fig.update_xaxes(side=\"bottom\")\n",
    "    fig.update_yaxes(autorange=True)\n",
    "\n",
    "    fig.update_coloraxes(cmid=0,\n",
    "#                          cmin=-30,\n",
    "#                          cmax= 30,\n",
    "                         colorscale='Portland',\n",
    "\n",
    "#                           colorscale=[[0.0, \"rgb(255,  0,  0)\"],\n",
    "#                                      [0.05, \"rgb(255,  0,  0)\"],\n",
    "#                                      [0.05, \"rgb(125,125,  0)\"],\n",
    "#                                      [0.1,  \"rgb(125,125,  0)\"],\n",
    "#                                      [0.1,  \"rgb(255,255,255)\"],\n",
    "# #                                      [0.2,  \"rgb(200,125,100)\"],\n",
    "# #                                      [0.2,  \"rgb(200,200,100)\"],\n",
    "# #                                      [0.3,  \"rgb(200,200,100)\"],\n",
    "# #                                      [0.3,  \"rgb(200,200,200)\"],\n",
    "# #                                      [0.5,  \"rgb(200,200,200)\"],\n",
    "#                                      [1.0,  \"rgb(155,155,155)\"]],\n",
    "                        colorbar_title_side='right',\n",
    "                        colorbar_tickfont=dict(size=18, ),\n",
    "                        colorbar_title_font=dict(size=18, ),\n",
    "                        reversescale=False,\n",
    "                         )\n",
    "\n",
    "    fig.update_layout(title_font_size=24,\n",
    "                      autosize=True,\n",
    "                      width=1200, height=900,\n",
    "                      margin=dict(l=10, r=10, b=10, t=40),\n",
    "                      )\n",
    "    fig.update_xaxes(title_font=dict(size=22, ), tickfont=dict(size=18), )\n",
    "    fig.update_yaxes(title_font=dict(size=22, ), tickfont=dict(size=18), )\n",
    "\n",
    "    if save_file:\n",
    "        print('saved ', output_fpn)\n",
    "#         fig.write_image(output_fpn)\n",
    "    else:\n",
    "        fig.show()\n",
    "\n",
    "def map_std_data(display_data, save_file=False, output_fpn=None):\n",
    "    # creating a accumulative map of dioptric landscape\n",
    "    fig = px.imshow(display_data,\n",
    "                    title=title_name,\n",
    "                    labels=dict(x='Horizontal decentration from fovea in degrees [°]',\n",
    "                                y='Vertical decentration from fovea in degrees [°]',\n",
    "                                color='Standard deviation [μm]',\n",
    "                                ),\n",
    "#                     # pixels to degrees translation\n",
    "                    x=np.array([float(x) * 0.0703 for x in range(0, 512)]) - 18,\n",
    "                    y=-1 * np.array([float(x) * 0.10547 for x in range(0, 256)]) + 13.5,\n",
    "#                         y = 1 * np.array([int(x) for x in range(0, 256)])\n",
    "                    )\n",
    "    fig.update_xaxes(side=\"bottom\")\n",
    "    fig.update_yaxes(autorange=True)\n",
    "\n",
    "    fig.update_coloraxes(\n",
    "                        colorscale='YlOrRd',\n",
    "\n",
    "                        colorbar_title_side='right',\n",
    "                        colorbar_tickfont=dict(size=18, ),\n",
    "                        colorbar_title_font=dict(size=18, ),\n",
    "                        reversescale=False,\n",
    "                         )\n",
    "\n",
    "    fig.update_layout(title_font_size=24,\n",
    "                      autosize=True,\n",
    "                      width=1200, height=900,\n",
    "                      margin=dict(l=10, r=10, b=10, t=40),\n",
    "                      )\n",
    "    fig.update_xaxes(title_font=dict(size=22, ), tickfont=dict(size=18), )\n",
    "    fig.update_yaxes(title_font=dict(size=22, ), tickfont=dict(size=18), )\n",
    "\n",
    "    if save_file:\n",
    "        print('saved ', output_fpn)\n",
    "#         fig.write_image(output_fpn)\n",
    "    else:\n",
    "        fig.show()\n",
    "        \n",
    "def map_point_inclusion_data(display_data, save_file=False, output_fpn=None):\n",
    "    # creating a accumulative map of dioptric landscape\n",
    "    fig = px.imshow(display_data,\n",
    "                    title=title_name,\n",
    "                    labels=dict(x='Horizontal decentration from fovea in degrees [°]',\n",
    "                                y='Vertical decentration from fovea in degrees [°]',\n",
    "                                color='n number of valid data per pixel positon',\n",
    "                                ),\n",
    "#                     # pixels to degrees translation\n",
    "                    x=np.array([float(x) * 0.0703 for x in range(0, 512)]) - 18,\n",
    "                    y=-1 * np.array([float(x) * 0.10547 for x in range(0, 256)]) + 13.5,\n",
    "#                         y = 1 * np.array([int(x) for x in range(0, 256)])\n",
    "                    )\n",
    "    fig.update_xaxes(side=\"bottom\")\n",
    "    fig.update_yaxes(autorange=True)\n",
    "\n",
    "    fig.update_coloraxes(\n",
    "                        colorscale='cividis',\n",
    "\n",
    "                        colorbar_title_side='right',\n",
    "                        colorbar_tickfont=dict(size=18, ),\n",
    "                        colorbar_title_font=dict(size=18, ),\n",
    "                        reversescale=False,\n",
    "                         )\n",
    "\n",
    "    fig.update_layout(title_font_size=24,\n",
    "                      autosize=True,\n",
    "                      width=1200, height=900,\n",
    "                      margin=dict(l=10, r=10, b=10, t=40),\n",
    "                      )\n",
    "    fig.update_xaxes(title_font=dict(size=22, ), tickfont=dict(size=18), )\n",
    "    fig.update_yaxes(title_font=dict(size=22, ), tickfont=dict(size=18), )\n",
    "\n",
    "    if save_file:\n",
    "        print('saved ', output_fpn)\n",
    "#         fig.write_image(output_fpn)\n",
    "    else:\n",
    "        fig.show()\n",
    "        \n",
    "        \n",
    "def appl_stats_on_map(map1):\n",
    "    p_values = np.empty((256, 512), dtype=float) \n",
    "\n",
    "    for row_idx in range(0,255): \n",
    "        for col_idx in range(0,511):\n",
    "            array1 = map1[:, row_idx, col_idx]\n",
    "#             array2 = map2[:, row_idx, col_idx]\n",
    "\n",
    "            p_values[row_idx, col_idx] = (stats.wilcoxon(array1, y=None)[1])\n",
    "    return p_values\n",
    "\n",
    "def map_pValues(display_data, save_file=False, output_fpn=None):\n",
    "    # creating a accumulative map of dioptric landscape\n",
    "    fig = px.imshow(display_data,\n",
    "                    title=title_name,\n",
    "                    labels=dict(x='Horizontal decentration from fovea in degrees [°]',\n",
    "                                y='Vertical decentration from fovea in degrees [°]',\n",
    "                                color=\"p-Values\",\n",
    "                                ),\n",
    "#                     # pixels to degrees translation\n",
    "                    x=np.array([float(x) * 0.0703 for x in range(0, 512)]) - 18,\n",
    "                    y=-1 * np.array([float(x) * 0.10547 for x in range(0, 256)]) + 13.5,\n",
    "#                         y = 1 * np.array([int(x) for x in range(0, 256)])\n",
    "                    )\n",
    "    fig.update_xaxes(side=\"bottom\")\n",
    "    fig.update_yaxes(autorange=True)\n",
    "\n",
    "    fig.update_coloraxes(\n",
    "                          colorscale=[[0.0, \"rgb(255,  0,  0)\"],\n",
    "                                     [0.05, \"rgb(255,  0,  0)\"],\n",
    "                                     [0.05, \"rgb(125,125,  0)\"],\n",
    "                                     [0.1,  \"rgb(125,125,  0)\"],\n",
    "                                     [0.1,  \"rgb(255,255,255)\"],\n",
    "#                                      [0.2,  \"rgb(200,125,100)\"],\n",
    "#                                      [0.2,  \"rgb(200,200,100)\"],\n",
    "#                                      [0.3,  \"rgb(200,200,100)\"],\n",
    "#                                      [0.3,  \"rgb(200,200,200)\"],\n",
    "#                                      [0.5,  \"rgb(200,200,200)\"],\n",
    "                                     [1.0,  \"rgb(155,155,155)\"]],\n",
    "                        colorbar_title_side='right',\n",
    "                        colorbar_tickfont=dict(size=18, ),\n",
    "                        colorbar_title_font=dict(size=18, ),\n",
    "                        reversescale=False\n",
    "                         )\n",
    "\n",
    "    fig.update_layout(title_font_size=30,\n",
    "                      autosize=True,\n",
    "                      width=1200, height=900,\n",
    "                      margin=dict(l=10, r=10, b=10, t=50),\n",
    "                      )\n",
    "    fig.update_xaxes(title_font=dict(size=22, ), tickfont=dict(size=28), )\n",
    "    fig.update_yaxes(title_font=dict(size=22, ), tickfont=dict(size=28), )\n",
    "\n",
    "    if save_file:\n",
    "        fig.write_image(output_fpn)\n",
    "    else:\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_ids = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 23, 26, ] # all px\n",
    "# px_ids = [1,    3,    5, 6, 7, 8,    10, 11, 12, 13, 14, 15,     17, 18, 20, 23, 26, ] # worstest removed \n",
    "# px_ids = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,     17, 18, 20, 23, 26, ] # 16 removed \n",
    "eye_id = 'OS'\n",
    "\n",
    "r_maps, nan_map = get_choroid_thickness_data(px_ids, eye_id)\n",
    "\n",
    "# creat images from residal choroid thickness \n",
    "output_fp = r'E:\\studyIII\\OCT_data\\images\\residuals'\n",
    "\n",
    "# # for control purpose only map source data of choroidal thickness residual maps \n",
    "# for idx, px_id in enumerate(px_ids):\n",
    "#     output_fn = str(px_id) + '_'+ eye_id + '_choroid_thickness_residual_pre_post.png'\n",
    "#     output_fpn = os.path.join(output_fp, output_fn)\n",
    "#     display_data = r_maps[idx, :,:]\n",
    "    \n",
    "#     title_name = ('Choroid thickness residual: '+ eye_id + ' , px_id: ' + str(px_id) +\n",
    "#                   ', avg: ' + str(np.round(np.nanmean(display_data), 2)) +\n",
    "#                   ', std: ' + str(np.round(np.nanstd(display_data), 2))  +\n",
    "#                   ', max: ' + str(np.round(np.nanmax(display_data), 2)) + \n",
    "#                   ', min: ' + str(np.round(np.nanmin(display_data), 2))        \n",
    "#                  ) \n",
    "#     map_significant_data(display_data, save_file=True, output_fpn=output_fpn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_data = nan_map\n",
    "# display_data[display_data>10] = 10  \n",
    "# fig = go.Figure(data=[go.Surface(z=display_data)])\n",
    "\n",
    "\n",
    "# fig.update_layout(scene_camera_eye=dict(y=-1, x=0.01, z=12),\n",
    "#                   scene_aspectmode='manual', \n",
    "#                   scene_aspectratio=dict(x=12, y=9, z=10),\n",
    "#                   scene = dict(xaxis = dict(nticks=10, range=[0,512],),\n",
    "#                                yaxis = dict(nticks=10, range=[0,256],),\n",
    "#                                zaxis = dict(nticks=10, range=[-30,30],),),)\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_data = np.abs(nan_map - np.max(nan_map))\n",
    "display_data[display_data <= 10] = 10\n",
    "title_name = ('Valid data per pixel position '+ eye_id + ', of available data:'\n",
    "                  ' avg: ' + str(np.round(np.nanmean(display_data), 2)) +\n",
    "                  ', std: ' + str(np.round(np.nanstd(display_data), 2))\n",
    "             )\n",
    "\n",
    "\n",
    "\n",
    "map_point_inclusion_data(display_data, save_file=False, output_fpn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_exclusion = 6\n",
    "display_data = np.nanmean(r_maps, axis=0)\n",
    "\n",
    "idxs = np.where(nan_map>=max_exclusion)\n",
    "display_data[idxs] = np.nan\n",
    "idxs = np.where(nan_map>=max_exclusion)\n",
    "display_data[idxs] = np.nan\n",
    "\n",
    "title_name = ('Mean choroid thickness change, ' + eye_id +\n",
    "              ', avg: ' + str(np.round(np.nanmean(display_data), 2)) +\n",
    "              ', std: ' + str(np.round(np.nanstd(display_data), 2))  +\n",
    "              ', max: ' + str(np.round(np.nanmax(display_data), 2)) + \n",
    "              ', min: ' + str(np.round(np.nanmin(display_data), 2))        \n",
    "             ) \n",
    "map_significant_data(display_data, save_file=False, output_fpn=None)\n",
    "fig_hist =plt.hist(display_data.flatten(), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_exclusion = 6\n",
    "display_data = np.nanstd(r_maps, axis=0)\n",
    "\n",
    "idxs = np.where(nan_map>=max_exclusion)\n",
    "display_data[idxs] = np.nan\n",
    "idxs = np.where(nan_map>=max_exclusion)\n",
    "display_data[idxs] = np.nan\n",
    "\n",
    "title_name = ('Standard deviation of residual, ' + eye_id +\n",
    "              ', avg: ' + str(np.round(np.nanmean(display_data), 2)) +\n",
    "              ', std: ' + str(np.round(np.nanstd(display_data), 2))  +\n",
    "              ', max: ' + str(np.round(np.nanmax(display_data), 2)) + \n",
    "              ', min: ' + str(np.round(np.nanmin(display_data), 2))        \n",
    "             )\n",
    "\n",
    "map_std_data(display_data, save_file=False, output_fpn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = appl_stats_on_map(r_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "display_data = p_values\n",
    "\n",
    "mask = np.ones((256, 512))\n",
    "idxs = np.where(nan_map>=max_exclusion)\n",
    "mask[idxs[0], idxs[1]] = 0\n",
    "idxs = np.where(nan_map>=max_exclusion)\n",
    "mask[idxs[0], idxs[1]] = 0\n",
    "idxs = np.where(mask == 0)\n",
    "\n",
    "display_data[idxs[0], idxs[1]] = np.nan\n",
    "\n",
    "im_output_fn = eye_id + '_all_wilcoxon_choroid_thickness_delta_reg_mm_outlierremoval.png'\n",
    "\n",
    "title_name = ( eye_id +', significance of choroid thickness change; Wilcoxon signed-rank test') \n",
    "map_pValues(p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_data = np.nanmean(r_maps, axis=0)\n",
    "\n",
    "idxs = np.where(nan_map>=max_exclusion)\n",
    "display_data[idxs] = np.nan\n",
    "idxs = np.where(nan_map>=max_exclusion)\n",
    "display_data[idxs] = np.nan\n",
    "\n",
    "idxs = np.where(p_values>0.05)\n",
    "display_data[idxs] = np.nan\n",
    "title_name = ('Choroid areas of significant change, ' + eye_id +\n",
    "              ', avg: ' + str(np.round(np.nanmean(display_data), 2)) +\n",
    "              ', std: ' + str(np.round(np.nanstd(display_data), 2))  +\n",
    "              ', max: ' + str(np.round(np.nanmax(display_data), 2)) + \n",
    "              ', min: ' + str(np.round(np.nanmin(display_data), 2))  \n",
    "             ) \n",
    "map_significant_data(display_data, save_file=False, output_fpn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "maxmin_value = 10\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6), dpi=300)\n",
    "\n",
    "d1 = np.nanmean(r_maps, axis=0)\n",
    "idxs = np.where(nan_map>=max_exclusion)\n",
    "d1[idxs] = np.nan\n",
    "idxs = np.where(nan_map>=max_exclusion)\n",
    "d1[idxs] = np.nan\n",
    "\n",
    "idxs = np.where(p_values<0.05)\n",
    "d1[idxs] = np.nan\n",
    "\n",
    "d1 = d1.flatten()\n",
    "\n",
    "d1[d1 >  maxmin_value] =  maxmin_value\n",
    "d1[d1 < -maxmin_value] = -maxmin_value\n",
    "\n",
    "\n",
    "\n",
    "d2 = np.nanmean(r_maps, axis=0)\n",
    "idxs = np.where(nan_map>=max_exclusion)\n",
    "d2[idxs] = np.nan\n",
    "idxs = np.where(nan_map>=max_exclusion)\n",
    "d2[idxs] = np.nan\n",
    "\n",
    "d2[d2 > maxmin_value] =  maxmin_value\n",
    "d2[d2< -maxmin_value] = -maxmin_value\n",
    "idxs = np.where(p_values>=0.05)\n",
    "d2[idxs] = np.nan\n",
    "\n",
    "d2 = d2.flatten()\n",
    "\n",
    "fig_hist =plt.hist((d1, d2) , 100, color=('grey', 'red'))\n",
    "plt.title(eye_id +', choroid thickness change data distriburion', size=18)\n",
    "plt.legend(['none significant data', 'statistically significant data'], \n",
    "           prop={'size':14}, frameon=False, loc='upper left')\n",
    "plt.xlabel('μm, 100 bins', size = 16)\n",
    "# plt.ylabel('my y label', size = 16)\n",
    "plt.xlim([-3.5, 8])\n",
    "plt.xticks(size = 16)\n",
    "plt.yticks(size = 16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(~np.isnan(d2))/(np.sum(~np.isnan(d1))+np.sum(~np.isnan(d2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_data = np.nanmean(r_maps, axis=0)\n",
    "\n",
    "left = display_data[0:256, 0:128][~np.isnan(display_data[0:256, 0:128])]\n",
    "\n",
    "centre = display_data[50: 200, 192:320][~np.isnan(display_data[50: 200, 192:320])]\n",
    "\n",
    "right = display_data[0:256, 384:512][~np.isnan(display_data[0:256, 384:512])]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 8), dpi=80)\n",
    "data = [left, centre, right]\n",
    "fig7, ax7 = plt.subplots(figsize=(12, 8), dpi=80)\n",
    "ax7.set_title('Effects of optical defocus on central (foveal) and peripheral retina')\n",
    "ax7.boxplot(data)\n",
    "plt.xticks([1, 2, 3], ['myopic defocus peripheral left', 'centre fovea area', 'hyperopic defocus peripheral right'])\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sumarize residual data from areas and record in table, to csv\n",
    "\n",
    "\n",
    "\n",
    "rscan_types_all = ['OD_residual', 'OS_residual', ]\n",
    "column_names = ['px_id', 'eye_id', \n",
    "                'overall_mean_raw', 'overall_std_raw', 'overall_nonenans_raw',\n",
    "                'overall_mean', 'overall_std', 'overall_nonenans',\n",
    "                'perc_data_incl', \n",
    "                'lhs_area_mean', 'lhs_area_std', 'lhs_valid_data', \n",
    "                'centre_area_mean', 'centre_area_std', 'centre_valid_data',\n",
    "                'rhs_area_mean', 'rhs_area_std', 'rhs_valid_data', \n",
    "               ]\n",
    "\n",
    "def summary_area_of_interest(arr1, arr2, px_id, eye_id, column_names):\n",
    "\n",
    "    df_new = pd.DataFrame(columns=column_names)\n",
    "    df_new.loc[0,'px_id'] = px_id\n",
    "    df_new.loc[0,'eye_id'] = eye_id\n",
    "    df_new.loc[0,'overall_mean_raw'] = np.nanmean(arr1)\n",
    "    df_new.loc[0,'overall_std_raw'] =  np.nanstd( arr1)                                         \n",
    "    df_new.loc[0,'overall_nonenans_raw'] =  np.sum(~np.isnan(arr1))\n",
    "\n",
    "    df_new.loc[0,'overall_mean'] = np.nanmean(arr2)\n",
    "    df_new.loc[0,'overall_std'] =  np.nanstd( arr2)                                         \n",
    "    df_new.loc[0,'overall_nonenans'] =  np.sum(~np.isnan(arr2))\n",
    "    \n",
    "    df_new.loc[0,'perc_data_incl'] = np.round(df_new.loc[0,'overall_nonenans'] / \n",
    "                                               df_new.loc[0,'overall_nonenans_raw'] * \n",
    "                                               100, 2)\n",
    "    \n",
    "    left = arr2[0:256, 0:128]\n",
    "    df_new.loc[0,'lhs_area_mean'] = np.nanmean(left)\n",
    "    df_new.loc[0,'lhs_area_std'] =  np.nanstd( left)                                         \n",
    "    df_new.loc[0,'lhs_valid_data'] = np.round(np.sum(~np.isnan(arr2[0:256, 0:128])) / \n",
    "                                              np.sum(~np.isnan(arr1[0:256, 0:128])) *\n",
    "                                              100, 2)\n",
    "    centre = arr2[50: 200, 192:320]\n",
    "    df_new.loc[0,'centre_area_mean'] = np.nanmean(centre)\n",
    "    df_new.loc[0,'centre_area_std'] =  np.nanstd(centre)\n",
    "    df_new.loc[0,'centre_valid_data'] = np.round(np.sum(~np.isnan(arr2[50: 200, 192:320])) / \n",
    "                                                 np.sum(~np.isnan(arr1[50: 200, 192:320])) *\n",
    "                                                 100, 2)\n",
    "    right = arr2[0:256, 384:512]\n",
    "    df_new.loc[0,'rhs_area_mean'] = np.nanmean(right)\n",
    "    df_new.loc[0,'rhs_area_std'] =  np.nanstd(right)\n",
    "    df_new.loc[0,'rhs_valid_data'] = np.round(np.sum(~np.isnan(arr2[0:256, 384:512])) / \n",
    "                                              np.sum(~np.isnan(arr1[0:256, 384:512])) *\n",
    "                                                 100, 2)\n",
    "\n",
    "    return df_new# sumarize data from areas and record in table, to csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sumarize data from areas and record in table, to csv\n",
    "px_ids = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 23, 26, ] # all px\n",
    "# px_ids = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,     17, 18, 20, 23, 26, ] # all px - 16 \n",
    "eye_id = 'OD'\n",
    "df_all = pd.DataFrame(columns=column_names)\n",
    "    \n",
    "for idx, px_id in enumerate(px_ids):\n",
    "    \n",
    "    # raw data\n",
    "    residuals_fn1 = str(px_id) +'_' + eye_id + '_choroid_thickness_map_mm_raw.csv'\n",
    "    fpn1 = os.path.join(residuals_data_fp, residuals_fn1)\n",
    "    arr1 = pd.read_csv(fpn1, index_col=0).values.astype(float) \n",
    "    # filtered data \n",
    "    residuals_fn2 = str(px_id) +'_' + eye_id + '_choroid_thickness_map_mm_std_filter.csv'\n",
    "    fpn2 = os.path.join(residuals_data_fp, residuals_fn2)\n",
    "    arr2 = pd.read_csv(fpn2, index_col=0).values.astype(float)  \n",
    "    # create summary of areas of interest \n",
    "    df_new = summary_area_of_interest(arr1, arr2, px_id, eye_id, column_names)  \n",
    "    \n",
    "    df_all = df_all.append(df_new)\n",
    "\n",
    "output_fn = eye_id + '_residuals_different_areas_maxstd6_incl_RX_AL.csv'\n",
    "df_all.to_csv(output_fn)    \n",
    "df_all   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('overall mean: ', np.nanmean(df_all.overall_mean), 'overall std: ', np.nanstd(df_all.overall_mean))\n",
    "\n",
    "\n",
    "print('left mean:    ',np.nanmean(df_all.lhs_area_mean), 'left std:    ',np.nanstd(df_all.lhs_area_mean))\n",
    "print('centrel mean: ',np.nanmean(df_all.centre_area_mean), 'centrel std: ',np.nanstd(df_all.centre_area_mean))\n",
    "print('right mean:   ',np.nanmean(df_all.rhs_area_mean), 'right std:   ',np.nanstd(df_all.rhs_area_mean))\n",
    "\n",
    "data_test = np.nanmean(r_maps, axis=0)\n",
    "idxs = np.where(nan_map>=max_exclusion)\n",
    "data_test[idxs] = np.nan\n",
    "idxs = np.where(nan_map>=max_exclusion)\n",
    "data_test[idxs] = np.nan\n",
    "\n",
    "\n",
    "print(np.nanmean(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = r'E:\\studyIII\\OCT_final_data'\n",
    "\n",
    "fn = '1_OS_choroid_thickness_map_mm_raw.csv'\n",
    "fpn = os.path.join(fp, fn)\n",
    "#         print(fpn)\n",
    "r_map_sample = pd.read_csv(fpn, index_col=0).values.astype(float) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "title_name = ('Residual data without filtering, ' + eye_id +\n",
    "              ', avg: ' + str(np.round(np.nanmean(r_map_sample), 2)) +\n",
    "              ', std: ' + str(np.round(np.nanstd(r_map_sample), 2))  +\n",
    "              ', max: ' + str(np.round(np.nanmax(r_map_sample), 2)) + \n",
    "              ', min: ' + str(np.round(np.nanmin(r_map_sample), 2))  \n",
    "             ) \n",
    "map_significant_data(r_map_sample\n",
    "                     , save_file=False, output_fpn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
